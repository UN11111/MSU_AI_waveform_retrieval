# MSU_AI_waveform_retrieval

# Разработка нейросетевой модели восстановления спектрально-временной формы терагерцевого (ТГц) импульса по сигналу ТГц-индуцированной втрой гармоники.
В рамках данного проекта предложен и продемонстрирован метод восстановления спектра и временной формы поля терагерцевого импульса на основе измерения сигнала второй оптической гармоники, генерируемой в газовой среде при двухцветном воздействии терагерцевого импульса и фемтосекундного инфракрасного лазерного излучения. Для восстановления спектрально-временной формы тергерцевого излучения использована предобученная на синтезированных данных полносвязная нейронная сеть, принимающая на вход измеряемую зависимость сигнала второй гармоники от задержки между инфракрасным и терагерцевым 
импульсами.

# Данные
В качестве данных для обучения и валидации используются синтезированные данные, представляющие собой набор зависимостей энергии второй гармоники от задержки между ТГц- и ИК-импульсами, подаваемых на вход нейросети, и соответствующих им параметров, описывающих спектрально-временную форму ТГц-импульса, представляющих собой выходные параметры нейросети. Для формирования устойчивости процесса восстановления на основе реальных экспериментальных сигналов в синтезированный сигнал добавляется шум с равномерным распределением в диапазоне ±2% от амплитуды сигнала второй гармоники, что отвечает реальной экспериментальной ситуации. Для организации процесса обучения нейросети проводилась нормировка данных с приведением их к диапазону [-1;1] и там, где это было более физически корректно, к диапазону [0;1]. 

# Методология
Задача восстановления спектрально-временной формы ТГц-импульса решалась в двух приближения. В рамках первого приближения для описания спектрально-временной формы импульса использовались три параметра, в рамках второго приближения - шесть. Для восстановления данных параметров, характеризующих форму импульса, использовалась нейронная сеть, обученная на синтезированых данных. Архитектура используемой нейросети представлена на

## Архитектура модели
На рисунке ниже представлена архитектура используемой нейросетевой модели.
![alt text](https://github.com/UN11111/MSU_AI_waveform_retrieval/blob/main/%D0%90%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0%20%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%BE%D0%B9%20%D1%81%D0%B5%D1%82%D0%B8.png)
## Гиперпараметры:
- Оптимизатор Adam;
- Функция потерь L1Loss (MAE);
- batch_size = 256 (1-е приближение) и 16384 (2-е приближение)
## Метрика качества
- MAE

Архитектура модели, а также функция для аугментации данных представлены в файле Model_and_trainer.py

## Результаты обучения

На протяжении 15 эпох сохраняется лучшая модель, затем для этой модели запускается новый цикл обучения на 15 эпох и также выбирается лучшая, и в конце понижается learning_rate до 0.00001 и производится окончательное обучение на 15 эпохах. Итоговая лучшая модель тестируется на тестовой выборке. Изначально для обучения использовалась метрика RMSE, val отбирался случайно, и каждый объект для обучения в каждой эпохе заново подгружался, затем удалялся. Так, процесс обучения был менее наглядным, так как не было понятно, в какую сторону "ошибается" алгоритм, в правильную или нет, и обучение длилось около 8 часов. Поэтому в последствии обучающая функция была оптимизирована и в качестве метрики включена корреляция Пирсона, так удалось в два раза ускорить обучение (с достижением того же качества предсказания на тестовых данных) и сделать его более наглядным.

#### Этап 1

#### Этап 2

#### Этап 3

Скрипт для обучения модели представлен в файле Train_Model.ipynb

### Метрики на тестовой выборке:
- Корреляция Пирсона = 0.6 (p-value = 4e-14)
- RMSE = 1.5

### Оценка важности признаков

Был рассчитан разброс весов, внутри каналов, которые в данном случае являются признаками. 

Наибольший разброс характерен для канала с ароматическими атомами и акцепторами и донорами водородных связей.

Скрипт для проверки модели на тестовой выборке и определения разброса признаков представлен в файле Test_model.ipynb

Работа выполнялась в среде Google Colab

 
